<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="songlh">







<title>【翻】Bias in AI is a mirror of our culture | LH&#39;BLOG</title>



    <link rel="icon" href="/favicon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    










  <meta name="generator" content="Hexo 6.2.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            <img loading="lazy" class="logo-img" src="/logo.png" alt="logo_image">
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/categories/gallery/">Posts</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/tags/Translate/">Translate</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/about/">About</a>
              </li> 
                   
          
          
            <li class="menu-item search-btn">
              <a href="#">Search</a>
            </li>
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
                
                    <span class="post-tag">
                        <a href="/tags/Translate/">
                            Translate
                        </a>
                    </span>    
                
                    <span class="post-tag">
                        <a href="/tags/AI/">
                            AI
                        </a>
                    </span>    
                           
            
        </div>
        <div class="post-title">
            
            
                【翻】Bias in AI is a mirror of our culture
            
            
        </div>
        <span class="post-date">
            7月 29, 2023
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/highlight.js/11.6.0/styles/base16/gruvbox-light-medium.min.css">
<script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<div class="post-content">
    <blockquote>
<p>为了提高英语水平和保持技术成长，开始按计划翻译一些短篇和博客，有问题欢迎讨论👻<br>原文：<a target="_blank" rel="noopener" href="https://uxdesign.cc/bias-in-ai-is-a-mirror-of-our-culture-3607bd795c57">Bias in AI is a mirror of our culture</a><br>原作者：<a target="_blank" rel="noopener" href="https://medium.com/@arvindsanjeev">Arvind Sanjeev</a></p>
</blockquote>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*ZQAjJr41_3iLqVKCfmyKGg.gif"></p>
<p>在今天的生活中，人类更多的是与 AI 模型互动，而非人类。无论是在 Youtube 和 Netflix 上进行娱乐活动，还是在 Instagram 和 TikTok 上寻找灵感，在 Tinder 和 Hinge 上寻找 朋友， AI 模型都会决定我们应该与哪些人类和内容互动。AI 模型已经在不知不觉地控制着我们的生活。</p>
<p>我们把决策权交给了算法，现在它们可以决定哪些人可以获得住房或医疗保险，谁会被雇佣或者被大学录取，我们的信用评分等等。人们希望通过公式将人类的看法剔除，使机器能够做出公平公正的决定，然而，训练这些模型的数据就是来自于一个从根本上存在偏见的社会；要完全消除这些偏见是不可能的。AI 只是将它们又反射给我们。</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/59bMh59JQDo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<h2 id="Algorithmic-racism"><a href="#Algorithmic-racism" class="headerlink" title="Algorithmic racism"></a>Algorithmic racism</h2><p><strong>种族歧视算法正在把人们送进监狱。</strong></p>
<p><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*CbF68Ttsv7AItzxNs_S0LA.png"></p>
<p>在2015年，Media Lab 的 Joy Buolamwini 发现，当时所有的人脸识别算法(来自 IBM 、 Microsoft 等公司)都无法检测到她的脸，因为她是一个黑人女性。然而，当她<a target="_blank" rel="noopener" href="https://artsandculture.google.com/story/BQWBaNKAVWQPJg?hl=en">带上白色面罩</a>时，这些算法又能看到她了。她注意到，在分析黑人女性和白人男性时，误差达到35%。我们过去还有许多其他的例子：</p>
<ul>
<li>在2015年，谷歌的图像标记算法把<a target="_blank" rel="noopener" href="https://www.theverge.com/2015/7/1/8880363/google-apologizes-photos-app-tags-two-black-people-gorillas">一对黑人朋友标为“大猩猩”</a></li>
<li>Flicker 的系统也犯了<a target="_blank" rel="noopener" href="https://petapixel.com/2015/05/20/flickr-fixing-racist-auto-tagging-feature-after-black-man-mislabeled-ape/">同样的错误</a>，用“动物”和“猿猴”标记了一个黑人。</li>
<li>Nikon 的相机可以检测一个人是否持续眨眼，但它却告诉一位亚洲用户<a target="_blank" rel="noopener" href="https://content.time.com/time/business/article/0,8599,1954643,00.html">她的眼睛是闭着的</a></li>
<li>HP 的网络摄像头在2009年可以轻松追踪到白人的脸，但却<a target="_blank" rel="noopener" href="https://gizmodo.com/hp-face-tracking-webcams-dont-recognize-black-people-5431190">看不到黑人的脸</a></li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/t4DT3tQqgRM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<blockquote>
<p>“We will not solve the problems of the present with the tools of the past. The past is a very racist place. And we only have data from the past to train Artificial Intelligence.”<br>— Trevor Paglen, artist and critical geographer</p>
</blockquote>
<p>有偏见的人脸识别算法甚至错误地将黑人男子逮捕并送进监狱。以下是美国执法人员使用种族歧视决策工具的两个例子：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">COMPAS</a>：美国法官和假释官使用的一种累犯预测软件，用于确定刑事被告的风险程度，进而影响他们的量刑。该算法不公平地将黑人和西班牙裔人归类为比其他人更危险的人。</li>
</ul>
<p><img src="https://miro.medium.com/v2/format:webp/1*jLw2AcMjDVAJ52em6SD6Pg.png"></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://predpol.com/">PrePol</a>: Predictive Policing 是一种“少数民族报告”式的犯罪预测软件，由警方用来确定哪些社区应该比其他社区进行更多的巡逻。由于这种有偏见的算法，黑人社区总是受到更多的巡逻和骚扰。</li>
</ul>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DMptJ7Eo9CG3XedP.png"></p>
<blockquote>
<p>Black men are six times more likely to be incarcerated by police than white men and 21 times more likely to be killed by them.<br>— Cathy O Neil, Weapons of Math Destruction</p>
</blockquote>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">This is a damn trap <a target="_blank" rel="noopener" href="https://t.co/UssHF5MtNk">pic.twitter.com/UssHF5MtNk</a></p>&mdash; Saturday Night Live - SNL (@nbcsnl) <a target="_blank" rel="noopener" href="https://twitter.com/nbcsnl/status/1502873834029989889?ref_src=twsrc%5Etfw">March 13, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="Gender-bias"><a href="#Gender-bias" class="headerlink" title="Gender bias"></a>Gender bias</h2><p><strong>Binary algorithms 加强了刻板印象</strong></p>
<p>Subspring 是有史以来第一个由 AI 创作的短片。Oscar Sharp用科幻电影脚本训练模型，创造出这部作品。2021年，当他尝试用动作电影剧本训练模型做类似的事情时却大吃一惊。因为他注意到，所有生成的故事都只有一个稳重的男性角色，而唯一的女性角色(恋爱对象)在整个故事中都只是被称为”女朋友”</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/sv6ew4EWRiQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>OpenAI的研究员发现了在自己的软件上描述男性和女性时产生的十大有偏见的词汇。对于男性来说，”large”、”mostly”、”lazy”、”fantastic “和 “eccentric “是最多的词汇。而对于女性来说，则是 “optimistic”、”bubbly”、”naughty”、”easy-going “和 “petite”。研究员写道：“我们发现，女性更常用以外表为导向的词汇描述自己”</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Machine learning models learn the biases back in data and reflect them back to us<br><br>Hungarians have no gendered pronouns but apparently, Google Translate has learnt all the gender stereotypes! 😱😱 <a target="_blank" rel="noopener" href="https://t.co/Hi0r62PMpF">pic.twitter.com/Hi0r62PMpF</a></p>&mdash; Bindu Reddy (@bindureddy) <a target="_blank" rel="noopener" href="https://twitter.com/bindureddy/status/1450317088271126529?ref_src=twsrc%5Etfw">October 19, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="Transphobic-algorithms"><a href="#Transphobic-algorithms" class="headerlink" title="Transphobic algorithms"></a>Transphobic algorithms</h2><p><strong>我们已经生活在一个性别歧视的世界里，但 AI 让情况变得更糟糕。</strong></p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAQr-ApTrxwBugIpd-Wmgg.png"></p>
<p>Automatic Gender Recognition(AGR)是 transphobic algorithms 的最佳示例。AGR广泛用于机场、商场等公共场所。当算法无法标记 non-binary 人的性别时，就会给他们带来很多羞辱和不适，从而引起他们更多的警觉和关注。</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Sejw17f93q7ItFEd1zH07A.png"></p>
<blockquote>
<p>“Every trans person you talk to has a TSA horror story. The best you can hope for is just to be humiliated; at worst, you are going to be harassed, detained, subjected to very invasive screenings, etc. Trans people or gender non-conforming people who have a multi-racial or Middle Eastern descent are most probable to have a negative airport security incident.”<br>— Kelsey Campbell, Founder Gayta Science</p>
</blockquote>
<p>了解更多 transphobic algorithms:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://edition.cnn.com/2019/11/21/tech/ai-gender-recognition-problem/index.html">AI software defines people as male or female. That’s a problem</a>。作者：Rachel Metz, CNN</li>
<li><a target="_blank" rel="noopener" href="https://venturebeat.com/ai/a-transgender-ai-researchers-nightmare-scenarios-for-facial-recognition-software/">A transgender AI researcher’s nightmare scenarios for facial recognition software</a>，作者：Khari Johnson，VentureBeat</li>
<li><a target="_blank" rel="noopener" href="https://reallifemag.com/counting-the-countless/">Counting the Countless</a>，作者：Os Keyes, Real life mag</li>
</ul>
<h2 id="Biased-hiring-algorithms"><a href="#Biased-hiring-algorithms" class="headerlink" title="Biased hiring algorithms"></a>Biased hiring algorithms</h2><p>如果你是一个叫 Jared 的白人，在高中打过曲棍球，你就能得到这份工作！</p>
<p>使用 AI-powered 简历筛选工具的灵感来自于<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Blind_audition">盲选</a>，即乐团在封闭帷幕后选择音乐家。这可以让他们忽略艺术家的性别、种族、声誉等因素。然而，大多数招聘算法不可避免地会<a target="_blank" rel="noopener" href="https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias">产生偏见</a>。</p>
<p>Amazon就是一个例子；他们的招聘和解雇算法都臭名昭著。他们的<a target="_blank" rel="noopener" href="https://qz.com/1427621/companies-are-on-the-hook-if-their-hiring-algorithms-are-biased">招聘算法偏爱白人男性</a>，而不是其他人群，而解雇算法则会在没有明确理由或者根本没有人为监督的情况下<a target="_blank" rel="noopener" href="https://www.bloomberg.com/news/features/2021-06-28/fired-by-bot-amazon-turns-to-machine-managers-and-workers-are-losing-out">解雇工厂工人</a>。</p>
<blockquote>
<p>After the company trained the algorithm on 10 years of its own hiring data, the algorithm reportedly became biased against female applicants. The word “women,” like in women’s sports, would cause the algorithm to specifically rank applicants lower.<br>— Companies are on the hook if their hiring algorithms are biased, QZ</p>
</blockquote>
<h2 id="Who’s-fighting-back"><a href="#Who’s-fighting-back" class="headerlink" title="Who’s fighting back?"></a>Who’s fighting back?</h2><p>AI 审计是最广为接受的减少偏见的技术之一。这个框架被广泛用于确定AI模型是否存在偏见、歧视或其他没发现的偏见。像Meta和Microsoft这种公司已经拥有内部审计团队来分析这些模型，但外部机构如ORCAA、ProPublica、NIST等机构也在进行审计。AJL（Algorithmic Justice League）的论文《Who audits the auditors》详细描述了AI审计生态系统的情况。</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/uaRKNGvQ4Pw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>以下是一些减少AI偏见的其他例子：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://nvlpubs.nist.gov/NISTpubs/SpecialPublications/NIST.SP.1270.pdf">Standards for Identifying and Managing Bias in Artificial Intelligence</a>：NIST制定的guideline，用于支持可信和负责性的AI发展</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.06921">Themis</a>：一款审计软件，用于设计出算法中的偏差，揭露出AI模型中隐藏的不平衡。</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer">Diffusion bias explorer</a>：来自HuggingFace的Sasha Luccioni的项目，揭示了Stable Diffusion中的隐藏偏见。</li>
<li>The <a target="_blank" rel="noopener" href="https://webtap.princeton.edu/">Princeton web accountability and transparency</a> project：这是一个利用机器人在网络上伪装成黑人、亚洲人、穷人、女性或跨性别者，并尝试在未被代表的数据集中代表这些少数群体的项目。</li>
<li><a target="_blank" rel="noopener" href="https://uxdesign.cc/bias-in-ai-is-a-mirror-of-our-culture-3607bd795c57">Feminist dataset</a>: 这个项目让社区参与创建数据集，以实现数据收集的透明方法。</li>
<li><a target="_blank" rel="noopener" href="https://www.ellpha.com/">Ellpha</a>：这个项目旨在创建性别平衡的数据集，使其能够真实地代表现实世界。</li>
<li><a target="_blank" rel="noopener" href="https://www.queerinai.com/">Queer in AI</a>：提高在 AI&#x2F;ML 领域同性恋问题的认识，培养同性恋研究人员和科学家群体。</li>
<li><a target="_blank" rel="noopener" href="https://www.gaytascience.com/">Gayta science</a>：利用数据科学技术捕捉、结合和提取见解，为LGBTQ+群体的经历赋予表达声音的机会。</li>
<li>Q：这是第一款自称雌雄同体的人的声音训练的无性别语音助手。这些声音的频率范围在145赫兹到175赫兹之间，正好位于男性和女性的声音范围之间的甜蜜点。</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/lvv6zYOQqm0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

<p>有偏见的算法被用作掩盖不公平做法和裁决少数群体的工具。通过以上的例子，希望你可以开始对 AI 所做的决策产生质疑。所以，下次当有人说这是一个由客观机器做出的公正决定时，就不会成为对话的终结者；相反，它会引起关于 AI 中偏见的全新讨论。</p>
<blockquote>
<p>High-tech tools have a built-in authority and patina of objectivity that often lead us to believe that their decisions are less discriminatory than those made by humans. But bias is introduced through programming choices, data selection, and performance metrics. It redefines social work as information processing, and then replaces social workers with computers. Humans that remain become extensions of algorithms.<br>— Virginia Eubanks, Automating Inequality</p>
</blockquote>
<p>有关此主题及其他主题的更多资源，请查阅本手册<a target="_blank" rel="noopener" href="https://arvindsanjeev.notion.site/arvindsanjeev/The-Handbook-of-AI-s-Unintended-Consequences-720cda3d8cec49dc88807641157a0bba">《AI’s unintended consequences》</a>。本文是探讨 <a target="_blank" rel="noopener" href="https://medium.com/@arvindsanjeev/ai-sociopaths-and-their-unintended-consequences-bf6f19012e4f">unintended consequences of AI 四部系列文章</a>中的第二章</p>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2023/08/05/%E3%80%90%E7%BF%BB%E3%80%91AI-is-sleepwalking-us-into-surveillance/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2023/07/21/%E3%80%90%E7%BF%BB%E3%80%91Indistinguishable-AI-content-is-crippling-our-perception/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    <!-- <div class="post-toc">
  <div class="post-toc-wrap">
    <div class="post-toc-title">目录</div>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E6%96%87"><span class="toc-text">正文</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithmic-racism"><span class="toc-text">Algorithmic racism</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gender-bias"><span class="toc-text">Gender bias</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transphobic-algorithms"><span class="toc-text">Transphobic algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Biased-hiring-algorithms"><span class="toc-text">Biased hiring algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Who%E2%80%99s-fighting-back"><span class="toc-text">Who’s fighting back?</span></a></li></ol></li></ol>
  </div>
</div>

<script>
  const tocLinks = document.querySelectorAll('.toc-link')
  tocLinks.forEach((el) => {
    el.setAttribute('href', `#${el.innerText}`)
  })
</script>
 -->
    
      <div class="post-comment">

     

    
        <div id="disqus_thread"></div>
        <script>
            /**
            *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
            *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
            
            var disqus_config = function () {
                this.page.url = 'https://lhrun.github.io/2023/07/29/%E3%80%90%E7%BF%BB%E3%80%91Bias-in-AI-is-a-mirror-of-our-culture/';  // Replace PAGE_URL with your page's canonical URL variable
                this.page.identifier = '2023/07/29/【翻】Bias-in-AI-is-a-mirror-of-our-culture/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                this.language = 'en'
            };
            
            (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://lhblog-1.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>   
     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>
                
        </div>
    </div>
</div>

    </div>

    
      <div class="search-popup">
    <div class="search-popup-overlay">  
    </div>
    <div class="search-popup-window" >
        <div class="search-header">
            <div class="search-input-container">
              <input autocomplete="off" autocapitalize="off" maxlength="80"
                     placeholder="Search Anything" spellcheck="false"
                     type="search" class="search-input">
            </div>
            <div class="search-close-btn">
                <div class="icon close-btn"></div>
            </div>
        </div>
        <div class="search-result-container">
        </div>
    </div>
</div>

<script>
    const searchConfig = {
        path             : "/search.xml",
        top_n_per_article: "1",
        unescape         : "false",
        trigger: "auto",
        preload: "false"
    }
</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js"></script>
<script src="/js/search.js"></script>
    
    

  </body>
</html>
